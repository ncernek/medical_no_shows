{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## high-level questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "<li>for a classification problem, how do i decide which models to try? Thus far I am just trying the ones I know.\n",
    "<li>all I do in this is try different models and try different features. is this the right method?\n",
    "<li>am i using the proper evaluation criteria?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 71735 entries, 0 to 71734\n",
      "Data columns (total 20 columns):\n",
      "Unnamed: 0             71735 non-null int64\n",
      "appointment_id         71735 non-null int64\n",
      "patient_id             71735 non-null float64\n",
      "scheduled_day          71735 non-null datetime64[ns]\n",
      "appointment_day        71735 non-null datetime64[ns]\n",
      "age                    71735 non-null int64\n",
      "neighborhood           71735 non-null object\n",
      "scholarship            71735 non-null int64\n",
      "hypertension           71735 non-null int64\n",
      "diabetes               71735 non-null int64\n",
      "alcoholism             71735 non-null int64\n",
      "handicap               71735 non-null int64\n",
      "sms_received           71735 non-null int64\n",
      "no_show                71735 non-null int64\n",
      "male                   71735 non-null int64\n",
      "appointment_count      71735 non-null int64\n",
      "days_to_appointment    71735 non-null int64\n",
      "scheduled_weekday      71735 non-null int64\n",
      "appointment_weekday    71735 non-null int64\n",
      "handicap_binary        71735 non-null int64\n",
      "dtypes: datetime64[ns](2), float64(1), int64(16), object(1)\n",
      "memory usage: 10.9+ MB\n"
     ]
    }
   ],
   "source": [
    "appointments = pd.read_csv(\"data/appointments_2.csv\",\n",
    "                          parse_dates=['scheduled_day','appointment_day'],\n",
    "                          )\n",
    "appointments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of features before creating dummies\n",
    "# this makes it easy to add and remove features\n",
    "features_initial = set(appointments.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the features from this list that cannot be used in any model,i.e.timestamps\n",
    "useless = {'Unnamed: 0', 'appointment_id', 'patient_id', 'scheduled_day',\n",
    "       'appointment_day','scheduled_weekday','no_show'}\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummies for categorical data: appointment day, neighborhood\n",
    "# only do this for categorical features we know have statistical significance\n",
    "appointments_dummies = pd.get_dummies(appointments, columns=['appointment_weekday','neighborhood'],drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-danger'>\n",
    "<li>is there a minimum category count above which I do not need to drop a column when doing dummies?\n",
    "<li>for example, do I need to drop a column for neighborhood to avoid multicollinearity?\n",
    "<li>is it a problem that I now have 80 columns for neighborhood? will this drown out the other columns significance?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_all = set(appointments_dummies.columns) - useless\n",
    "features_dummies = features_all - features_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without derived data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this first model is simply to establish a baseline. How does a model that only uses predictors from the initial dataset perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the data for training and testing\n",
    "columns = features_all - {\n",
    "    'days_to_appointment', \n",
    "    'appointment_weekday', \n",
    "    'handicap_binary'\n",
    "}\n",
    "\n",
    "X = appointments[list(columns)]\n",
    "y = appointments['no_show']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# train the model\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      1.00      0.83     10256\n",
      "          1       0.00      0.00      0.00      4091\n",
      "\n",
      "avg / total       0.51      0.71      0.60     14347\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nico/code/pydata-env/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predictions = logmodel.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model allows me to play with various categorical predictors without managing all the dummy predictors. I assume that toggling anyone of these will have more of an impact than toggling an individual dummy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the data for training and testing\n",
    "\n",
    "predictors_2 = {\n",
    "    'days_to_appointment', \n",
    "    'scholarship', \n",
    "#     'hypertension', \n",
    "#     'diabetes', \n",
    "#     'sms_received',\n",
    "    'age',\n",
    "#     'appointment_weekday', \n",
    "#     'handicap_binary'\n",
    "}\n",
    "    \n",
    "\n",
    "X = appointments[list(predictors_2)]\n",
    "y = appointments['no_show']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# train the model\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      1.00      0.83     10256\n",
      "          1       0.37      0.00      0.00      4091\n",
      "\n",
      "avg / total       0.62      0.71      0.60     14347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = logmodel.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take into account the optimizations made in the previous regression model by turning off certain predictors and including all of the dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors_3 = predictors - {\n",
    "    'hypertension', \n",
    "    'diabetes', \n",
    "    'sms_received',\n",
    "    'appointment_weekday', \n",
    "    'handicap_binary',\n",
    "    'male',\n",
    "    'handicap'\n",
    "}\n",
    "\n",
    "X = appointments_dummies[list(predictors_3)]\n",
    "y = appointments_dummies['no_show']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# train the model\n",
    "logmodel = LogisticRegression()\n",
    "logmodel.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.72      1.00      0.83     10256\n",
      "          1       0.43      0.01      0.01      4091\n",
      "\n",
      "avg / total       0.63      0.71      0.60     14347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = logmodel.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "how do I toggle the dummy predictors given that there are so many?\n",
    "<li>the easiest ways to do this manually. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# random forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all features with statistically significant distinctions between categories\n",
    "predictors_4 = {\n",
    "    'days_to_appointment', \n",
    "    'scholarship', \n",
    "    'hypertension', \n",
    "    'diabetes', \n",
    "    'sms_received',\n",
    "    'age',\n",
    "    'appointment_weekday', \n",
    "    'handicap_binary'\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## without dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "is it appropriate to assume that the optimal predictors for the logistic regression are the same for random forests?\n",
    "<li>No. Outside of the features that have strong correlations with the response, different models use features differently.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=600)\n",
    "\n",
    "X = appointments[list(predictors_4)]\n",
    "y = appointments['no_show']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "rfc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data based on fitted model\n",
    "predictions = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.84      0.78     10256\n",
      "          1       0.36      0.22      0.27      4091\n",
      "\n",
      "avg / total       0.62      0.66      0.64     14347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8612 1644]\n",
      " [3180  911]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with optimal predictor list based on logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=600)\n",
    "\n",
    "X = appointments_dummies[list(predictors_3)]\n",
    "y = appointments_dummies['no_show']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "rfc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data based on fitted model\n",
    "predictions = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.88      0.81     10256\n",
      "          1       0.44      0.23      0.30      4091\n",
      "\n",
      "avg / total       0.65      0.70      0.66     14347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9044 1212]\n",
      " [3150  941]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "would it make my model stronger if I got rid of the neighborhood columns that had a very low count?\n",
    "<li>No. it is not worth the effort to compress these features.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "X = appointments_dummies[list(predictors_3)]\n",
    "y = appointments_dummies['no_show']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# train the model\n",
    "svc_model = SVC()\n",
    "svc_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svc_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a note on evaluation methods\n",
    "source: [sklearn docs](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)\n",
    "\n",
    "### precision\n",
    "The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. In layman terms, it is how little your model claims that a bad burger is actually a tasty burger. The lower your false positives, the higher your precision.\n",
    "* the real question is why this is called precision. it should be called: **avoiding false positives**\n",
    "\n",
    "### recall\n",
    "The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples. In layman terms, it is how little your model throws away tasty burgers. The lower your false negatives, the higher your recall.\n",
    "* this should be called: **avoiding false negatives**\n",
    "\n",
    "### f-beta\n",
    "The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n",
    "* we use harmonic mean here because we are taking the average of fractions\n",
    "* The F-beta score weights recall more than precision by a factor of beta. beta == 1.0 means recall and precision are equally important.\n",
    "\n",
    "### support\n",
    "The support is the number of occurrences of each class in y_true."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "562px",
    "left": "0px",
    "right": "1067.01px",
    "top": "67px",
    "width": "213px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
